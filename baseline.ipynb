{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class model:\n",
    "    def __init__(self, df, num_labels, model_id='bert-base-cased', ckpt_dir='ckpts', num_epochs=3):\n",
    "        \"\"\"\n",
    "        General Model Class for task\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): {\"text\":str, \"label\":int}\n",
    "            num_labels (int): number of labels in the task\n",
    "            model_id (str, optional): model_id of HuggingFace model to use. Defaults to \"bert-base-cased\".\n",
    "            ckpt_dir (str, optional): checkpoint dir to store the checkpoints. Defaults to 'ckpts'.\n",
    "            num_epochs (int, optional): number of epochs to train the model. Defaults to 3.\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.df = df\n",
    "        self.num_labels = num_labels\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_id, num_labels=self.num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=self.ckpt_dir, \n",
    "            evaluation_strategy=\"epoch\", \n",
    "            num_train_epochs=self.num_epochs,\n",
    "        )\n",
    "        self.metric = evaluate.load(\"f1\")\n",
    "\n",
    "    def tokenize_function(self, data):\n",
    "        return self.tokenizer(data[\"text\"], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return self.metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    def train(self):\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(self.df['text'], self.df['label'], test_size=0.1, random_state=42)\n",
    "        train_data = Dataset.from_dict({\n",
    "            'text': X_train.values,\n",
    "            'label': y_train.values\n",
    "        })\n",
    "        eval_data = Dataset.from_dict({\n",
    "            'text': X_eval.values,\n",
    "            'label': y_eval.values\n",
    "        })\n",
    "        tokenized_train_dataset = train_data.map(self.tokenize_function, batched=True)\n",
    "        tokenized_eval_dataset = eval_data.map(self.tokenize_function, batched=True)\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=tokenized_train_dataset,\n",
    "            eval_dataset=tokenized_eval_dataset,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )   \n",
    "        result = trainer.train()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_baseline(df):\n",
    "    df.drop(['rewire_id', 'label_category', 'label_vector'], axis=1, inplace=True)    \n",
    "    df['label_sexist'].replace({'not sexist':0, 'sexist':1}, inplace=True)\n",
    "    df.rename(columns={'label_sexist':'label'}, inplace=True)\n",
    "    print(df.head(10))\n",
    "    obj = model(df, 2)\n",
    "    result = obj.train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0              Damn, this writing was pretty chaotic      0\n",
      "1  Yeah, and apparently a bunch of misogynistic v...      0\n",
      "2       How the FUCK is this woman still an MP!!!???      0\n",
      "3  Understand. Know you're right. At same time I ...      0\n",
      "4     Surprized they didn't stop and rape some women      0\n",
      "5  yeah but in order to keep the benefit i have t...      0\n",
      "6  Hahaha I just wish they would live their truth...      0\n",
      "7  head mod said she talked to him after he made ...      0\n",
      "8  just the inside of your wallet: cash, bank car...      1\n",
      "9  We most definitely did. I didnâ€™t go too much i...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9652a3e75242c4a6c5514643ca791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bdb052825444db843b72e5f4c3f540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/users/jtripathy/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 12600\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4725\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='127' max='4725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 127/4725 00:11 < 06:45, 11.35 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/train_all_tasks.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m binary_baseline(df)\n",
      "\u001b[1;32m/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb Cell 4\u001b[0m in \u001b[0;36mbinary_baseline\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m obj \u001b[39m=\u001b[39m model(df, \u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m result \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32m/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb Cell 4\u001b[0m in \u001b[0;36mmodel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m tokenized_eval_dataset \u001b[39m=\u001b[39m eval_data\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     args\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m )   \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bim/home/users/jtripathy/project/Explainable-Sexism/baseline.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1794\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1791\u001b[0m         model\u001b[39m.\u001b[39mclip_grad_norm_(args\u001b[39m.\u001b[39mmax_grad_norm)\n\u001b[1;32m   1792\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1793\u001b[0m         \u001b[39m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[39;00m\n\u001b[0;32m-> 1794\u001b[0m         nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[1;32m   1795\u001b[0m             amp\u001b[39m.\u001b[39;49mmaster_params(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer) \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_apex \u001b[39melse\u001b[39;49;00m model\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m   1796\u001b[0m             args\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[1;32m   1797\u001b[0m         )\n\u001b[1;32m   1799\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/utils/clip_grad.py:32\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(parameters, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     31\u001b[0m     parameters \u001b[39m=\u001b[39m [parameters]\n\u001b[0;32m---> 32\u001b[0m parameters \u001b[39m=\u001b[39m [p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     33\u001b[0m max_norm \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(max_norm)\n\u001b[1;32m     34\u001b[0m norm_type \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(norm_type)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/utils/clip_grad.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(parameters, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     31\u001b[0m     parameters \u001b[39m=\u001b[39m [parameters]\n\u001b[0;32m---> 32\u001b[0m parameters \u001b[39m=\u001b[39m [p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     33\u001b[0m max_norm \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(max_norm)\n\u001b[1;32m     34\u001b[0m norm_type \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(norm_type)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparameters\u001b[39m(\u001b[39mself\u001b[39m, recurse: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   1500\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \n\u001b[1;32m   1502\u001b[0m \u001b[39m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \n\u001b[1;32m   1519\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_parameters(recurse\u001b[39m=\u001b[39mrecurse):\n\u001b[1;32m   1521\u001b[0m         \u001b[39myield\u001b[39;00m param\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1546\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \n\u001b[1;32m   1542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[1;32m   1544\u001b[0m     \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39m_parameters\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1545\u001b[0m     prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse)\n\u001b[0;32m-> 1546\u001b[0m \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m gen:\n\u001b[1;32m   1547\u001b[0m     \u001b[39myield\u001b[39;00m elem\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1491\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m   1489\u001b[0m modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_modules(prefix\u001b[39m=\u001b[39mprefix) \u001b[39mif\u001b[39;00m recurse \u001b[39melse\u001b[39;00m [(prefix, \u001b[39mself\u001b[39m)]\n\u001b[1;32m   1490\u001b[0m \u001b[39mfor\u001b[39;00m module_prefix, module \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m-> 1491\u001b[0m     members \u001b[39m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   1492\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m members:\n\u001b[1;32m   1493\u001b[0m         \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m v \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1544\u001b[0m, in \u001b[0;36mModule.named_parameters.<locals>.<lambda>\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnamed_parameters\u001b[39m(\u001b[39mself\u001b[39m, prefix: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, recurse: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Tuple[\u001b[39mstr\u001b[39m, Parameter]]:\n\u001b[1;32m   1524\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39m    name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \n\u001b[1;32m   1542\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_named_members(\n\u001b[0;32m-> 1544\u001b[0m         \u001b[39mlambda\u001b[39;00m module: module\u001b[39m.\u001b[39;49m_parameters\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1545\u001b[0m         prefix\u001b[39m=\u001b[39mprefix, recurse\u001b[39m=\u001b[39mrecurse)\n\u001b[1;32m   1546\u001b[0m     \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m gen:\n\u001b[1;32m   1547\u001b[0m         \u001b[39myield\u001b[39;00m elem\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_all_tasks.csv')\n",
    "binary_baseline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
